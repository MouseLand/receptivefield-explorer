{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# maximally activating stimulus\n",
    "\n",
    "This notebook creates a stimulus that optimally drives each neuron. A deep learning model is trained to predict neural response; then, gradient ascent is used to optimize an image to maximally activate each node in the final layer corresponding to each neuron. The result is a set of images, one for each neuron, that maximally activates each neuron.\n",
    "\n",
    "For a more detailed description on maximally activating stimuli, check out the paper \"Inception loops discover what excites neurons using deep predictive models.\" [(Link.)](https://www.nature.com/articles/s41593-019-0517-x?proof=t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part one: train deep neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, mkl  \n",
    "mkl.set_num_threads(10)\n",
    "import numpy as np \n",
    "from scipy import io \n",
    "from matplotlib import pyplot as plt \n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and process data with rastermap\n",
    "\n",
    "Here, we use [rastermap](https://github.com/MouseLand/rastermap) clustering to preprocess the data into 1000 \"super neurons.\" The maximally activating stimuli will correspond to each superneuron.\n",
    "\n",
    "This step is not necessary to create maximally activating neurons, which can alternatively be created directly from the neural data, without rastermap preprocessing. Alternatively, you can use k-means clustering or some other preprocessing technique before creating the maximally activating stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/groups/pachitariu/pachitariulab/datasets/txt5k_3x/'\n",
    "dat = np.load(path + 'TX61_3x.npz') # neural data\n",
    "dstim = io.loadmat(path + 'text5k_3x.mat', squeeze_me=True) # stimulus data\n",
    "spks = dat['spks']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original spks shape (69957, 32468)\n"
     ]
    }
   ],
   "source": [
    "print(\"original spks shape\", spks.shape) # neurons x timepoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test splits for testing rastermap\n",
    "import sys\n",
    "sys.path.append('/groups/stringer/home/josephs2/rastermap')\n",
    "import rastermap\n",
    "from rastermap.mapping_landmark import Rastermap\n",
    "from rastermap.utils import split_testtrain\n",
    "itrain, itest = split_testtrain(spks.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_PCs = 200 computed, time 34.91\n"
     ]
    }
   ],
   "source": [
    "from rastermap import mapping_landmark\n",
    "model = mapping_landmark.Rastermap(n_clusters=200).fit(spks, itrain=itrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rastermap.metrics import embedding_score\n",
    "mnn, rho = embedding_score(model.X_test, model.embedding)\n",
    "print('local/global scores: %2.3f; %2.3f'%(mnn[0], rho))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin rastermap by neurons\n",
    "nbins = 1000\n",
    "\n",
    "y = model.embedding[:,0]\n",
    "isort = np.argsort(y)\n",
    "imax = np.arange(y.size)//(y.size/nbins)\n",
    "\n",
    "NT = spks.shape[1]\n",
    "xbin = np.zeros((nbins, NT))\n",
    "for j in range(nbins):\n",
    "    xbin[j] = spks[isort[imax==j]].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort xbin into stimuli and average over the three repeats\n",
    "iss = np.zeros((3,5000), 'int32')\n",
    "for j in range(5000):\n",
    "    iss[:,j] = (dat['istim']==j).nonzero()[0][:3]\n",
    "    \n",
    "x = xbin[:, dat['frame_start']]\n",
    "x = x[:, iss].transpose((1,0,2))\n",
    "\n",
    "xzz = zscore(x, 2)\n",
    "snr = (xzz[0] * xzz[1]).mean(1) # this is the stimulus correlation across repeats, or \"SNR\"\n",
    "\n",
    "x = x.mean(0)\n",
    "x = zscore(x, 1)\n",
    "\n",
    "print(\"x shape\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess image data \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import cv2\n",
    "\n",
    "nb = 100\n",
    "Ly, Lx = 24, 84\n",
    "img = np.zeros((5000, Ly, Lx))\n",
    "for j in range(5000//nb):\n",
    "    img[j*nb:(j+1)*nb] = cv2.resize(dstim['img'][:,:,j*nb:(j+1)*nb], (2*42, 2*12)).transpose((2,0,1))\n",
    "\n",
    "# all the receptive fields are on the front left monitor\n",
    "img = img[:,:,40:-10]\n",
    "img = img - img.mean(0) # center the data\n",
    "img = img / (img**2).mean() **.5\n",
    "\n",
    "nimg, Ly, Lx = img.shape\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split into training and test for the deep neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "\n",
    "X = img.reshape(-1, 1, 24, 34)\n",
    "Y = x.T\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = [torch.Tensor(x) for x in train_test_split(X, Y, random_state=42, shuffle=True)] # train/test\n",
    "X_train, X_val, y_train, y_val = [torch.Tensor(x) for x in train_test_split(X_train, y_train, random_state=42, shuffle=True)] # train/validation\n",
    "\n",
    "print(\"\\nX_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"\\ny_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True, num_workers=96)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=True, num_workers=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create deep neural net (CNN)\n",
    "\n",
    "We use a simple CNN here, but you can put your own deep learning model in this section. The maximally activating stimuli generation process is the same across deep neural nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "# use these functions to calculate the shape and size of the CNN outputs\n",
    "def shape_of_output(shape_of_input, list_of_layers):\n",
    "    sequential = nn.Sequential(*list_of_layers)\n",
    "    return tuple(sequential(torch.rand(1, *shape_of_input)).shape)\n",
    "\n",
    "def size_of_output(shape_of_input, list_of_layers):\n",
    "    return functools.reduce(operator.mul, list(shape_of_output(shape_of_input, list_of_layers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  pick a CNN\n",
    "We can either go with the simple 1-layer CNN, the multi-layer CNN, or a deep learning model of your own design.\n",
    "\n",
    "**Remember to comment out the CNNs you are not using!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_neurons (int): Number of neurons in final layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_neurons, channels=30, kernel_size=4, padding=2, stride=1, max_pool=4):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.num_neurons = num_neurons\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, channels, kernel_size=kernel_size, padding=padding, stride=stride),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(max_pool),\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(size_of_output(X_train[0].shape, self.cnn), self.num_neurons, bias=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = [32, 32, 32, 32] # number of channels of hidden conv2d layers\n",
    "\n",
    "class MultiCNN(nn.Module):\n",
    "    def __init__(self, num_neurons=1000, k = 7, c_in = 1, c_out = 32, size_pool = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(c_in, kk[0], kernel_size=k, padding=k//2, stride=1) # n x 8\n",
    "        self.batch1 = nn.BatchNorm2d(kk[0])\n",
    "        self.conv2 = nn.Conv2d(kk[0], kk[1], kernel_size=3, padding=1, stride=1) # n x 8\n",
    "        self.batch2 = nn.BatchNorm2d(kk[1])\n",
    "        self.conv3 = nn.Conv2d(kk[1], kk[2], kernel_size=3, padding=1, stride=1) # n x 8\n",
    "        self.batch3 = nn.BatchNorm2d(kk[2])\n",
    "        self.conv4 = nn.Conv2d(kk[2], kk[3], kernel_size=3, padding=1, stride=1) # n x 8\n",
    "        self.batch4 = nn.BatchNorm2d(kk[3])\n",
    "\n",
    "        self.batch_o2 = nn.BatchNorm2d(kk[2])\n",
    "        self.fc2 = nn.Linear(self.get_shape(X_train)[1], num_neurons, bias=True)\n",
    "        \n",
    "    def get_shape(self, x):\n",
    "        x = F.relu(self.batch1(self.conv1(x)))\n",
    "        x = F.avg_pool2d(x, 2)\n",
    "\n",
    "        x = x + F.relu(self.batch2(self.conv2(x)))\n",
    "        x = F.avg_pool2d(x, 2)\n",
    "\n",
    "        x = x + F.relu(self.batch3(self.conv3(x)))\n",
    "        x = F.avg_pool2d(x, 2)\n",
    "\n",
    "        x = x + F.relu(self.batch4(self.conv4(x)))\n",
    "        x = self.batch_o2(x)\n",
    "        \n",
    "        x = x.reshape( -1, x.shape[1] * x.shape[2]*x.shape[3])\n",
    "        return x.shape\n",
    "        \n",
    "\n",
    "    def forward(self, x, fc=False):\n",
    "        x = F.relu(self.batch1(self.conv1(x)))\n",
    "        x = F.avg_pool2d(x, 2)\n",
    "\n",
    "        x = x + F.relu(self.batch2(self.conv2(x)))\n",
    "        x = F.avg_pool2d(x, 2)\n",
    "\n",
    "        x = x + F.relu(self.batch3(self.conv3(x)))\n",
    "        x = F.avg_pool2d(x, 2)\n",
    "\n",
    "        x = x + F.relu(self.batch4(self.conv4(x)))\n",
    "        x = self.batch_o2(x)\n",
    "        \n",
    "        x = x.reshape( -1, x.shape[1] * x.shape[2]*x.shape[3])\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    a = torch.square(x[:, :, :-1, :] - x[:, :, 1:, :]).mean((-2,-1))\n",
    "    b = torch.square(x[:, :, :, :-1] - x[:, :, :, 1:]).mean((-2,-1))\n",
    "    loss = (torch.pow(a + b, 1.25)).mean(-1)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "def pearson(pred, true):\n",
    "    \"\"\"\n",
    "    Neuron-by-neuron Pearson's correlation to measure\n",
    "    overall score.\n",
    "    \n",
    "    Args:\n",
    "        pred \n",
    "        true\n",
    "    \"\"\"\n",
    "    total_pearson = defaultdict(float)\n",
    "    for i in range(pred.shape[1]):\n",
    "        r, p = pearsonr(pred[:,i], true[:,i])\n",
    "        total_pearson[i] = r\n",
    "    return np.mean(list(total_pearson.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train deep neural net\n",
    "\n",
    "We train a deep neural net for 200 epochs, but you are encouraged to tailor the hyperparameters to your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from collections import defaultdict\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# net=CNN(num_neurons=1000)\n",
    "net = MultiCNN(num_neurons=1000)\n",
    "\n",
    "epochs = 100\n",
    "optimizer = optim.SGD(net.parameters(), lr=6e-5, momentum=0.9,\n",
    "                     weight_decay=.01)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "train_loss, train_acc, val_loss, val_acc = [], [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Training\n",
    "    running_train_loss = 0.0\n",
    "    running_pearson = 0.0\n",
    "    total = 0\n",
    "    t = tqdm(train_loader)\n",
    "    t.set_description(\"Epoch {} - Training\".format(epoch))\n",
    "    for data in t:\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        running_train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += 1   \n",
    "        \n",
    "        p = pearson(outputs.detach().numpy(), labels.detach().numpy())\n",
    "        running_pearson += p\n",
    "        \n",
    "    scheduler.step() \n",
    "    \n",
    "    epoch_loss = running_train_loss / total\n",
    "    epoch_acc = running_pearson / total\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_acc.append(epoch_acc)\n",
    "    \n",
    "    # Validation\n",
    "    if epoch % 15 == 0:\n",
    "        print(\"Epoch {}. Train loss {}. Train Pearson's {}\".format(epoch, epoch_loss, epoch_acc))\n",
    "\n",
    "        running_val_loss = 0.0\n",
    "        running_pearson = 0.0\n",
    "        total = 0 \n",
    "        v = tqdm(val_loader)\n",
    "        v.set_description(\"Validating\")\n",
    "        for data in v:\n",
    "            inputs, labels = data\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            p = pearson(outputs.detach().numpy(), labels.detach().numpy())\n",
    "            running_pearson += p\n",
    "            total += 1\n",
    "\n",
    "        epoch_loss = running_val_loss / total\n",
    "        epoch_acc = running_pearson / total\n",
    "        val_loss.append(epoch_loss)\n",
    "        val_acc.append(epoch_acc)\n",
    "\n",
    "        print(\"Epoch {}. Validation loss {}. Validation Pearson's {}\".format(epoch, epoch_loss, epoch_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### view loss curves\n",
    "\n",
    "We plot the loss to make sure that the training process was effective. If training was successful, our loss should should decrease, then plateau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(range(len(train_loss)), train_loss, label=\"train loss\")\n",
    "interval = int(len(train_loss) / len(val_loss)) + 1\n",
    "plt.plot(range(0, len(train_loss), interval), val_loss, label=\"val loss\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Pearson\")\n",
    "plt.plot(range(len(train_acc)), train_acc, label=\"train acc\")\n",
    "plt.plot(range(0, len(train_acc), interval), val_acc, label=\"val acc\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### view neuron-by-neuron pearson's correlation\n",
    "\n",
    "We calculate the Pearson's correlation for each neuron (node by node of the final layer of the neural net) by comparing the test data with our neural net's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model\n",
    "from collections import defaultdict\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Evaluate model\n",
    "device='cuda'\n",
    "net.eval()\n",
    "net.to(device)\n",
    "batch_size=4\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False, num_workers=96)\n",
    "\n",
    "y_pred_matrix = []\n",
    "y_test_matrix = []\n",
    "for batch in test_loader:\n",
    "    x_te, y_te = batch\n",
    "    x_te, y_te = x_te.to(device), y_te.to(device)\n",
    "    y_pred = net(x_te)\n",
    "    y_pred_matrix.append(y_pred.detach().cpu())\n",
    "    y_test_matrix.append(y_te)\n",
    "\n",
    "y_pred_matrix, y_test_matrix = torch.cat(y_pred_matrix).cpu().numpy(), torch.cat(y_test_matrix).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_pearson = defaultdict(float)\n",
    "print(\"Calculating pearson's for\", y_pred_matrix.shape[1], \"clusters\")\n",
    "for n in range(y_pred_matrix.shape[1]):\n",
    "    r, p = pearsonr(y_pred_matrix[:,n], y_test_matrix[:,n])\n",
    "    total_pearson[n] = r\n",
    "\n",
    "len(total_pearson)\n",
    "values = total_pearson.values()\n",
    "plt.figure()\n",
    "hist = plt.hist(values, bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Mean pearson's {}\".format(np.mean(list(values))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part two: create maximally activating stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create maximally activating stimuli function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class VisualizeNeuron():\n",
    "    def __init__(self, model):\n",
    "        self.device = \"cuda\"\n",
    "        self.model = model.to(self.device).eval()\n",
    "    \n",
    "    def visualize(self, neuron, Lyd, Lxd, iterations=10e4, lr=.3):\n",
    "\n",
    "        # Generate random image\n",
    "        img_var = np.uint(np.random.uniform(150, 180, (Lyd, Lxd))) / 255\n",
    "        img_var = img_var.reshape(1, Lyd, Lxd)\n",
    "            \n",
    "        # Run gradient ascent\n",
    "        img_var = Variable(torch.Tensor(img_var[None]), requires_grad=True)\n",
    "        iterations = int(10e4)\n",
    "        total_loss = []\n",
    "        for i in range(iterations):\n",
    "            if i / 1000 == 0:\n",
    "                print(\"On iteration: {}\".format(i))\n",
    "            output = self.model(img_var.to(self.device))\n",
    "            loss = -output[:,neuron]\n",
    "            total_loss.append(loss)\n",
    "            optimizer = torch.optim.Adam([img_var], lr=lr, weight_decay=1e-6)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        img_var = img_var.data.cpu().numpy()\n",
    "        return img_var, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot maximally activating stimuli and visualize loss\n",
    "\n",
    "As with training the original net, ensure that the loss curve plateaus out, so that we know training is converging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lyd, Lxd = 24, 34\n",
    "img, total_loss = VisualizeNeuron(model=net).visualize(0, Lyd, Lxd, iterations=1000, lr=.5)\n",
    "plt.figure()\n",
    "plt.imshow(img.reshape(Lyd, Lxd), cmap=\"gray\")\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, total_loss = VisualizeNeuron(model=net).visualize(0, Lyd, Lxd, iterations=1000, lr=.1)\n",
    "plt.figure()\n",
    "plt.imshow(img.reshape(Lyd, Lxd), cmap=\"gray\")\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot most predictive neurons based on Pearson's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get most predictive neurons based on Pearson's\n",
    "from scipy.stats import pearsonr\n",
    "# total_r = []\n",
    "# for i,j in zip(dat['y_test'], dat['y_pred']):\n",
    "#     r, p = pearsonr(i, j)\n",
    "#     total_r.append(r)\n",
    "indices = np.argsort(list(values))[::-1]\n",
    "# indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_picked = 10\n",
    "# neuron_list = np.random.choice(NN, size=neurons_picked)\n",
    "# neuron_list \n",
    "neuron_list = indices[:neurons_picked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn = VisualizeNeuron(net)\n",
    "total_stimuli = []\n",
    "total_loss = []\n",
    "for neuron in neuron_list:\n",
    "    print(\"On neuron {}\".format(neuron))\n",
    "    img, loss = vn.visualize(neuron, Lyd, Lxd, iterations=500, lr=.3)\n",
    "    total_stimuli.append(img)\n",
    "    total_loss.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "for j, (n,stim) in enumerate(zip(neuron_list,total_stimuli)):\n",
    "    vl = np.max(np.abs(stim))\n",
    "    plt.subplot(4,4,j+1)\n",
    "    plt.imshow(stim.reshape(Lyd, Lxd), cmap='gray')\n",
    "    plt.title(\"neuron {}\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "for j, (n, loss) in enumerate(zip(neuron_list, total_loss)):\n",
    "    plt.subplot(4,4,j+1)\n",
    "    plt.plot(loss)\n",
    "    plt.title(str(\"neuron {}\".format(n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 3: optional analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [optional] save data for future use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save neural net\n",
    "\n",
    "# SET NAME\n",
    "NAME = # enter name here\n",
    "\n",
    "# SET OUTPUT DIRECTORY\n",
    "torch.save({\"model_state_dict\": net.state_dict()}, \"save_models/\" + NAME)\n",
    "print(\"Model saved.\")\n",
    "new_dat = {}\n",
    "new_dat['X_train'] = X_train\n",
    "new_dat['y_train'] = y_train\n",
    "new_dat['X_val'] = X_val\n",
    "new_dat['y_val'] = y_val\n",
    "new_dat['X_test'] = X_test\n",
    "new_dat['y_test'] = y_test\n",
    "new_dat['y_pred'] = y_pred_matrix\n",
    "np.save(\"save_models/\" + NAME + \"_DATA\", new_dat)\n",
    "print(\"Data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "import torch\n",
    "\n",
    "model_path = \"save_models/\" + NAME\n",
    "checkpoint = torch.load(model_path)\n",
    "# prepare model\n",
    "cnn_model = CNN(num_neurons=1000)\n",
    "cnn_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [optional] view neural trace\n",
    "\n",
    "We plot out a neural trace of our most predictive neuron to get a sense of the accuracy of our deep learning model. We can see that our current model does not capture the neuron's highest spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(y_test_matrix[:150,0], label=\"test\")\n",
    "plt.plot(y_pred_matrix[:150,0], label=\"pred\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(y_test_matrix[:,0], label=\"test\")\n",
    "plt.plot(y_pred_matrix[:,0], label=\"pred\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### top-performing neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top performing neuron\n",
    "indices = np.argsort(list(values))[::-1]\n",
    "\n",
    "n = indices[0]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(y_test_matrix[:150,n], label=\"test\")\n",
    "plt.plot(y_pred_matrix[:150,n], label=\"pred\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(y_test_matrix[:,n], label=\"test\")\n",
    "plt.plot(y_pred_matrix[:,n], label=\"pred\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [optional] view rastermap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for rastermap display\n",
    "xz = xbin.copy()\n",
    "xz = xz - np.mean(xz, 0)\n",
    "xz = zscore(xz, 1)\n",
    "\n",
    "# # for receptive field display\n",
    "# rf = rfs[200]\n",
    "# vv = 2 * np.maximum(rf.max(), -rf.min())\n",
    "# nn = 90\n",
    "\n",
    "\n",
    "# in order: a) rastermap, b) stim SNR, c) regression corr d) RFs (sub-sampled), e) all consecutive RFs (positions 220-310)\n",
    "plt.figure(figsize=(16,24))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(xz[:, 5000:5200], vmin=0, vmax=2, cmap = 'gray_r', aspect='auto')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 24, 12)\n",
    "plt.barh(np.arange(len(snr))[::-1], snr, 1, color = [.4, .2,.4])\n",
    "plt.axis('off')\n",
    "plt.ylim([0, len(snr)-1])\n",
    "plt.xlim([0, .8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rastermap position in tissue\n",
    "xpos = dat['xpos']\n",
    "ypos = dat['ypos']\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.scatter(xpos, -ypos, s = 1, c = model.embedding[:,0]/200, cmap = 'gist_ncar', alpha=.5)\n",
    "plt.colorbar()\n",
    "plt.xlabel('x position (um)', fontsize=12)\n",
    "plt.ylabel('y position (um)', fontsize=12)\n",
    "\n",
    "#plt.savefig('D:/Github/rastercode/fig/xypos_fov_rmap.png', bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [optional] visualize filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 17))\n",
    "for i, fil in enumerate(net.conv1.weight):\n",
    "    plt.subplot(8, 8, i+1) # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)\n",
    "    plt.imshow(fil[0, :, :].detach().cpu(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
